{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Retrieve-inspection-jobs-from-Ceph\" data-toc-modified-id=\"Retrieve-inspection-jobs-from-Ceph-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Retrieve inspection jobs from Ceph</a></span></li><li><span><a href=\"#Describe-the-structure-of-an-inspection-job-result\" data-toc-modified-id=\"Describe-the-structure-of-an-inspection-job-result-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Describe the structure of an inspection job result</a></span></li><li><span><a href=\"#Mapping-InspectionRun-JSON-to-pandas-DataFrame\" data-toc-modified-id=\"Mapping-InspectionRun-JSON-to-pandas-DataFrame-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Mapping InspectionRun JSON to pandas DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance-analysis\" data-toc-modified-id=\"Feature-importance-analysis-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Feature importance analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Status\" data-toc-modified-id=\"Status-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Status</a></span></li><li><span><a href=\"#Specification\" data-toc-modified-id=\"Specification-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Specification</a></span></li><li><span><a href=\"#Job-log\" data-toc-modified-id=\"Job-log-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Job log</a></span></li></ul></li></ul></li><li><span><a href=\"#Profile-InspectionRun-duration\" data-toc-modified-id=\"Profile-InspectionRun-duration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Profile InspectionRun duration</a></span></li><li><span><a href=\"#Plot-InspectionRun-duration\" data-toc-modified-id=\"Plot-InspectionRun-duration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Plot InspectionRun duration</a></span></li><li><span><a href=\"#Library-usage\" data-toc-modified-id=\"Library-usage-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Library usage</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amun InspectionRun Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "The goal of this notebook is to show the behaviour of micro-benchmarks tested on the selected software/hardware architecture. This notebook will evaluate the feasibility and accuracy of the tests in order to prove or discard the possibility of running benchmarks tests for ML applications. \n",
    "\n",
    "The analysis consider ~300 inspection jobs obtained considering:\n",
    "\n",
    "- same libraries\n",
    "- same versions\n",
    "- same environment\n",
    "\n",
    "The inputs for test are shown in section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve inspection jobs from Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env THOTH_DEPLOYMENT_NAME     thoth-core-upshift-stage\n",
    "%env THOTH_CEPH_BUCKET         thoth\n",
    "%env THOTH_CEPH_BUCKET_PREFIX  data/thoth\n",
    "%env THOTH_S3_ENDPOINT_URL     https://s3.upshift.redhat.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoth.storages import InspectionResultsStore\n",
    "\n",
    "inspection_store = InspectionResultsStore()\n",
    "inspection_store.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id, doc = next(inspection_store.iterate_results())  # sample\n",
    "\n",
    "# build log is unnecessary for our purposes and it is demanding to display it\n",
    "doc['build_log'] = None\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the structure of an inspection job result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structure_json(input_json, upper_key: str, level: int, json_structure):\n",
    "    \"\"\"Convert a json file structure into a list with rows showing tree depths, keys and values\"\"\"\n",
    "    level += 1\n",
    "    for key in input_json.keys():\n",
    "        if type(input_json[key]) is dict:\n",
    "            json_structure.append([level, upper_key, key, [k for k in input_json[key].keys()]])\n",
    "            \n",
    "            extract_structure_json(input_json[key], f\"{upper_key}__{key}\", level, json_structure)\n",
    "        else:\n",
    "            json_structure.append([level, upper_key, key, input_json[key]])\n",
    "            \n",
    "    return json_structure\n",
    "\n",
    "def filter_dfs(df_s, filter_df):\n",
    "    \"\"\"Filter the specific dataframe created for a certain key, combination of keys or for a tree depth\"\"\"\n",
    "    if type(filter_df) is str:\n",
    "        available_keys = set(df_s[\"Current_key\"].values)\n",
    "        available_combined_keys = set(df_s[\"Upper_keys\"].values)\n",
    "        \n",
    "        if filter_df in available_keys:\n",
    "            ndf = df_s[df_s[\"Current_key\"].str.contains(f\"^{filter_df}$\", regex=True)]\n",
    "            \n",
    "        elif filter_df in available_combined_keys:\n",
    "            ndf = df_s[df_s[\"Upper_keys\"].str.contains(f\"{filter_df}$\", regex=True)]\n",
    "        else:\n",
    "            print(\"The key is not in the json\")\n",
    "            ndf = \"\". join([f\"The available keys are (WARNING: Some of the keys have no leafs):{available_keys} \", f\"The available combined keys are: {available_combined_keys}\"])\n",
    "    elif type(filter_df) is int:\n",
    "        max_depth = df_s[\"Tree_depth\"].max()\n",
    "        if filter_df <= max_depth:\n",
    "            ndf = df_s[df_s[\"Tree_depth\"] == filter_df]\n",
    "        else:\n",
    "            ndf = f\"The maximum tree depth available is: {max_depth}\"\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the inspection job structure from the point of view of the tree depth, considering a key or a combination of keys in order to understand the common inputs for all inspections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structure = pd.DataFrame(extract_structure_json(doc, \"\", 0, []))\n",
    "df_structure.columns = [\"Tree_depth\", \"Upper_keys\", \"Current_key\", \"Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check hardware info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CPU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo__cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Platform information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo__platform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which libraries are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for package in filter_dfs(df_structure, \"__specification__python__requirements_locked__default\")[\"Current_key\"].values:\n",
    "    dfp = filter_dfs(df_structure, f\"__specification__python__requirements_locked__default__{package}\")\n",
    "    print(\"{:15}  {}\".format(package, dfp[dfp[\"Current_key\"].str.contains(\"version\")][\"Value\"].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping InspectionRun JSON to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "require": [
     "notebook/js/codecell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pandas_profiling import ProfileReport as profile\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspection_results = []\n",
    "\n",
    "for document_id, document in inspection_store.iterate_results():\n",
    "    # pop build logs to save some memory (not necessary for now)\n",
    "    document['build_log'] = None\n",
    "    \n",
    "    inspection_results.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_normalize(inspection_results, sep = \".\")  # each row resembles InspectionResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance analysis\n",
    "\n",
    "For the purposes of the performance analysis we take under consideration the impact of a variable on the performance score, the variance of the features is therefore an important indicator. We can assume that the more variance feature evinces, the higher is its impact on the performance measure stability.\n",
    "\n",
    "We can perform profiling as the first stage of this analysis to identify constants which won't affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"The original DataFrame contains  {len(df.columns)}  columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top-level keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspection_results[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "require": [
     "base/js/events",
     "datatables.net",
     "d3",
     "jupyter-datatables"
    ]
   },
   "outputs": [],
   "source": [
    "df_status = df.filter(regex='status')\n",
    "\n",
    "date_columns = df_status.filter(regex=\"started_at|finished_at\").columns\n",
    "for col in date_columns:\n",
    "    df_status[col] = df[col].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = profile(df_status)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the profiling, we can drop the values with the constant value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = p.description_set['variables'].query(\"distinct_count <= 1 & type != 'UNSUPPORTED'\")\n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = df.filter(regex='specification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = profile(df_spec)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = p.description_set['variables'].query(\"distinct_count <= 1 & type != 'UNSUPPORTED'\")\n",
    "rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exclude versions, we might wanna use them later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = rejected.filter(regex=\"^((?!version).)*$\", axis=0)\n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = df.filter(regex='job_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = profile(df_job)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = p.description_set['variables'].query(\"distinct_count <= 1 & type != 'UNSUPPORTED'\")\n",
    "    \n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "\n",
    "def process_inspection_results(\n",
    "    inspection_results: List[dict],\n",
    "    exclude: Union[list, set] = None,\n",
    "    apply: List[Tuple] = None,\n",
    "    verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process inspection result into pd.DataFrame.\"\"\"\n",
    "    if not inspection_results:\n",
    "        return ValueError(\"Empty iterable provided.\")\n",
    "    \n",
    "    exclude = exclude or []\n",
    "    apply = apply or ()\n",
    "    \n",
    "    df = json_normalize(inspection_results, sep = \"__\")  # each row resembles InspectionResult\n",
    "    \n",
    "    if len(df) <= 1:\n",
    "        return df\n",
    "    \n",
    "    for regex, func in apply:\n",
    "        for col in df.filter(regex=regex).columns:\n",
    "            df[col] = df[col].apply(func)\n",
    "    \n",
    "    keys = [k for k in inspection_results[0] if not k in exclude]\n",
    "    for k in keys:\n",
    "        if k in exclude:\n",
    "            continue\n",
    "        d = df.filter(regex=k)\n",
    "        p = profile(d)\n",
    "        \n",
    "        rejected = p.description_set['variables'] \\\n",
    "            .query(\"distinct_count <= 1 & type != 'UNSUPPORTED'\") \\\n",
    "            .filter(regex=\"^((?!version).)*$\", axis=0)  # explicitly include versions\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Rejected columns: \", rejected.index)\n",
    "        \n",
    "        df.drop(rejected.index, axis=1, inplace=True)\n",
    "        \n",
    "    df = df \\\n",
    "        .eval(\"status__job__duration   = status__job__finished_at   - status__job__started_at\", engine='python') \\\n",
    "        .eval(\"status__build__duration = status__build__finished_at - status__build__started_at\", engine='python')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_inspection_results(\n",
    "    inspection_results,\n",
    "    exclude=['build_log', 'created', 'inspection_id'],\n",
    "    apply=[\n",
    "        (\"created|started_at|finished_at\", pd.to_datetime)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile InspectionRun duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration = df.filter(like='duration') \\\n",
    "    .rename(columns=lambda s: s.replace(\"status__\", \"\").replace(\"__\", \"_\")) \\\n",
    "    .apply(lambda ts: pd.to_timedelta(ts).dt.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = profile(df_duration)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = p.description_set['variables'].drop([\"histogram\", \"mini_histogram\"], axis=1)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot InspectionRun duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the versions are constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(regex='python.*version').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_duration.iplot(\n",
    "    kind='box',\n",
    "    title=\"InspectionRun duration\",\n",
    "    yTitle=\"duration [s]\",\n",
    "    asFigure=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration \\\n",
    "    .iplot(\n",
    "    fill='tonexty',\n",
    "    kind='scatter',\n",
    "    title=\"InspectionRun duration\",\n",
    "    yTitle=\"duration [s]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration = df_duration \\\n",
    "    .eval(\"job_duration_mean           = job_duration.mean()\", engine='python') \\\n",
    "    .eval(\"build_duration_mean         = build_duration.mean()\", engine='python') \\\n",
    "    .eval(\"job_duration_upper_bound    = job_duration + job_duration.std()\", engine='python') \\\n",
    "    .eval(\"job_duration_lower_bound    = job_duration - job_duration.std()\", engine='python') \\\n",
    "    .eval(\"build_duration_upper_bound  = build_duration + build_duration.std()\", engine='python') \\\n",
    "    .eval(\"build_duration_lower_bound  = build_duration - build_duration.std()\", engine='python')\n",
    "\n",
    "upper_bound = go.Scatter(\n",
    "    name='Upper Bound',\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration_upper_bound,\n",
    "    mode='lines',\n",
    "    marker=dict(color=\"lightgray\"),\n",
    "    line=dict(width=0),\n",
    "    fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "    fill='tonexty' )\n",
    "\n",
    "trace = go.Scatter(\n",
    "    name='Duration',\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration,\n",
    "    mode='lines',\n",
    "    line=dict(color='rgb(31, 119, 180)'),\n",
    "    fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "    fill='tonexty' )\n",
    "\n",
    "lower_bound = go.Scatter(\n",
    "    name='Lower Bound',\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration_lower_bound,\n",
    "    marker=dict(color=\"lightgray\"),\n",
    "    line=dict(width=0),\n",
    "    mode='lines')\n",
    "\n",
    "data = [lower_bound, trace, upper_bound]\n",
    "\n",
    "m = stats.loc['job_duration']['mean']\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(title='duration [s]'),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': 0,\n",
    "            'x1': len(df_duration.index),\n",
    "            'y0': m,\n",
    "            'y1': m,\n",
    "            'line': {\n",
    "                'color': 'red',\n",
    "                'dash': 'longdash'\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    title='InspectionRun job duration',\n",
    "    showlegend = False)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='pandas-time-series-error-bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.lib.histograms._hist_bin_auto(df_duration.job_duration.values, None)\n",
    "\n",
    "df_duration.job_duration.iplot(\n",
    "    title=\"InspectionRun job distribution\",\n",
    "    xTitle=\"duration [s]\",\n",
    "    yTitle=\"count\",\n",
    "    kind='hist',\n",
    "    bins=int(np.ceil(bins))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Library usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import cufflinks as cf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas_profiling import ProfileReport as profile\n",
    "\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "\"\"\"Thoth InspectionRun dashboard app.\"\"\"\n",
    "\n",
    "cf.go_offline()\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option('colheader_justify', 'center')\n",
    "\n",
    "\n",
    "def create_duration_dataframe(inspection_df: pd.DataFrame):\n",
    "    \"\"\"Compute statistics and duration DataFrame.\"\"\"\n",
    "    if len(inspection_df) <= 0:\n",
    "        raise ValueError(\"Empty DataFrame provided\")\n",
    "\n",
    "    try:\n",
    "        inspection_df.drop(\"build_log\", axis=1, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    data = (\n",
    "        inspection_df.filter(like=\"duration\")\n",
    "        .rename(columns=lambda s: s.replace(\"status__\", \"\").replace(\"__\", \"_\"))\n",
    "        .apply(lambda ts: pd.to_timedelta(ts).dt.total_seconds())\n",
    "    )\n",
    "    data = (\n",
    "        data\n",
    "        .eval(\"job_duration_mean           = job_duration.mean()\", engine=\"python\")\n",
    "        .eval(\"job_duration_upper_bound    = job_duration + job_duration.std()\", engine=\"python\")\n",
    "        .eval(\"job_duration_lower_bound    = job_duration - job_duration.std()\", engine=\"python\")\n",
    "        .eval(\"build_duration_mean         = build_duration.mean()\", engine=\"python\")\n",
    "        .eval(\"build_duration_upper_bound  = build_duration + build_duration.std()\", engine=\"python\")\n",
    "        .eval(\"build_duration_lower_bound  = build_duration - build_duration.std()\", engine=\"python\")\n",
    "    )\n",
    "\n",
    "    return data.round(4)\n",
    "\n",
    "\n",
    "def create_duration_box(data: pd.DataFrame, columns: List[str] = None, **kwargs):\n",
    "    \"\"\"Create duration Box plot.\"\"\"\n",
    "    columns = columns or []\n",
    "\n",
    "    figure = data[columns].iplot(\n",
    "        kind=\"box\", title=kwargs.pop(\"title\", \"InspectionRun duration\"), yTitle=\"duration [s]\", asFigure=True\n",
    "    )\n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def create_duration_scatter(data: pd.DataFrame, col: str, **kwargs):\n",
    "    \"\"\"Create duration Scatter plot.\"\"\"\n",
    "    df_duration = (\n",
    "        data[[col]]\n",
    "        .eval(f\"upper_bound = {col} + {col}.std()\", engine=\"python\")\n",
    "        .eval(f\"lower_bound = {col} - {col}.std()\", engine=\"python\")\n",
    "    )\n",
    "\n",
    "    upper_bound = go.Scatter(\n",
    "        name=\"Upper Bound\",\n",
    "        x=df_duration.index,\n",
    "        y=df_duration.upper_bound,\n",
    "        mode=\"lines\",\n",
    "        marker=dict(color=\"lightgray\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "        fill=\"tonexty\",\n",
    "    )\n",
    "\n",
    "    trace = go.Scatter(\n",
    "        name=\"Duration\",\n",
    "        x=df_duration.index,\n",
    "        y=df_duration[col],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgb(31, 119, 180)\"),\n",
    "        fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "        fill=\"tonexty\",\n",
    "    )\n",
    "\n",
    "    lower_bound = go.Scatter(\n",
    "        name=\"Lower Bound\",\n",
    "        x=df_duration.index,\n",
    "        y=df_duration.lower_bound,\n",
    "        marker=dict(color=\"lightgray\"),\n",
    "        line=dict(width=0),\n",
    "        mode=\"lines\",\n",
    "    )\n",
    "\n",
    "    data = [lower_bound, trace, upper_bound]\n",
    "    m = df_duration[col].mean()\n",
    "\n",
    "    layout = go.Layout(\n",
    "        yaxis=dict(title=\"duration [s]\"),\n",
    "        shapes=[\n",
    "            {\n",
    "                \"type\": \"line\",\n",
    "                \"x0\": 0,\n",
    "                \"x1\": len(df_duration.index),\n",
    "                \"y0\": m,\n",
    "                \"y1\": m,\n",
    "                \"line\": {\"color\": \"red\", \"dash\": \"longdash\"},\n",
    "            }\n",
    "        ],\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun duration\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_duration_histogram(data: pd.DataFrame, columns: List[str] = None, bins: int = None, **kwargs):\n",
    "    \"\"\"Create duration histogram.\"\"\"\n",
    "    columns = columns or data.columns\n",
    "\n",
    "    if not bins:\n",
    "        bins = np.max([np.lib.histograms._hist_bin_auto(data[col].values, None) for col in columns])\n",
    "\n",
    "    figure = data[columns].iplot(\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun distribution\"),\n",
    "        xTitle=\"duration [s]\",\n",
    "        yTitle=\"count\",\n",
    "        kind=\"hist\",\n",
    "        bins=int(np.ceil(bins)),\n",
    "        asFigure=True,\n",
    "    )\n",
    "\n",
    "    return figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_inspection_results(\n",
    "    inspection_results,\n",
    "    exclude=['build_log', 'created', 'inspection_id'],\n",
    "    apply=[\n",
    "        (\"created|started_at|finished_at\", pd.to_datetime)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration = create_duration_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_duration_box(df_duration, ['build_duration', 'job_duration'])\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_duration_scatter(df_duration, 'job_duration', title=\"InspectionRun job duration\")\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_duration_scatter(df_duration, 'build_duration', title=\"InspectionRun build duration\")\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_duration_histogram(df_duration, ['job_duration'])\n",
    "\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
